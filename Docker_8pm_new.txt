==================================================================
Day 1
==================================================================

Docker
===================

Virtaulization
===================
	Here we a have a baremetal (H/W) on which we install the 
host OS and on the host OS we install an application  called as
hypervisor,on this hypervisor we can install guest OS and on the
guest OS we can run the s/w applications that we want
This technology enables us to run multiple OS's parallely on one
server.
The disdavantage is these applications have to pass through so many 
layers in order to access the H/W resources.
Similary each VM requires fixed amout of H/W to be allocated


Containarization
==========================
	Here we have a bare metal on top of which we install the host
OS and on the host OS we install an application called as Docker Engine
On this docker  engine we can run any application in the form of container
These containers are extrememly light weight and consume very less amount
of H/W resources.They can be created very quickly comared to VM's

Docker is used for "process isolation" ie the depndency that an application 
has on an OS is removed and they can directly run on top of docker engine.
The advantage is we nedd not spend money on licensing of the OS's
Docker can be used at all the below stages of s/w development
Build--->Ship--->Run


Installing docker on Windows
======================================
1 Open https://docs.docker.com/docker-for-windows/install/
2 Download docker fow windows---->Install it
3 To execute docker command use "PowerShell"

Note: Docker can be installed on on Windows 10 Prof 64 bit version 
or Windows 2016 Server edition

Note: Once docker is installed on Windows it activates an application
called Windows hypervisor(HyperV).Once this is activated it will not allow
any other virtualization s/w to run

=======================================================================
Day 2
=======================================================================
Setup of Ubuntu Server using vagrant
---------------------------------------
Vagrant
===================
1 Download and install Oracle virtual Box
  https://www.virtualbox.org/wiki/Downloads

2 Download and install vagrant
  https://www.vagrantup.com/downloads.html

3 To check if vagrant is installed
  vagrant --version

4 Copy the vagrantfile into an empty folder

5 Open cmd prompt
  cd path_of_folder_where_vagrantfile_is_copied
  vagrant up

6 Open Oracle virtual box to access these VM's
  Username and Password: vagrant

7 To install docker
  a)Download the shell script for docker
    curl -fsSL https://get.docker.com -o get-docker.sh
  b)Execute the shell script
    sh get-docker.sh


======================================================================
Setup of docker on a linux machine on AWS
=========================================
1 Create an AWS ubuntu20 instance
2 Connect to it using git bash
3 Download the shell script for docker
  curl -fsSL https://get.docker.com -o get-docker.sh
4 Install docker
  sh get-docker.sh


Url: http://get.docker.com

=============================================================================


Image and Containers
=============================
An image is a combination of bins/libs that are used to for
a specific s/w application

A running isntances of an image is called as a container

Docker host: The server where docker is installed is called
as docker host

Docker client:This is a s/w application that is responsible for accepting
the docker commands from the user and pass them to a process called docker
deamon

Docker deamon: This is a background process that accepts the commands from
 the docker client and routes them to work on images or containers or
registry

Registry: This is the location where the docker images are stored
This if 2 types
1 Public
2 Private

=========================================================================
Day 3
=========================================================================
Important Docker commands
==============================

Working on Docker images
=============================
1 To download a  docker image
  docker pull image_name

2 To search for a docker images
  docker search image_name

3 To see the list of docker images downloaded
  docker images
  (or)
  docker image ls

4 To delete a docker image
  docker rmi image_name/image_id

5 To get detailed info about a docker image
  docker inspect image_name/image_id

6 To create an image from a container
  docker commit container_id/container_name image_name

7 To create an image from a dockerfile
  docker build -t image_name .
 
8 To save a docker image a tar file
  docker image save image_name/image_id

9 To untar a tarred image and make it as a regular docker image
  docker image load tar_filename

10 To delete all images
   docker image prune

11 To upload docker images into docker registry
   docker push image_name

===========================================================================
Working on docker containers
=================================
12 To create a docker container
   docker run image_name
   Run command options
   ----------------------
   --name:  Used to give a name for the container
   -d :  USed to run the container in detached mode in the background
   -e : Used to pass environment varibles to container
   -p : Used to do port mapping ie the container port(internal port) is 
        linked with the host port(external port)
        Eg: -p 8080:80  Here 8080 is the host port and 80 is the container port
   -P : Used for automatic port mapping ie it will link the container port
        with a host port that is greater than 30000
   --link : Used for linking multiple containers to create a multi container
            architecture
   --network : Used to start a container in a specific custom network
   -v : Used to mount an external directory or device as volume to 
        preserve the container data
   --volume-from : Used to share volumes between multiple containers
   -m : Used to give an upper limit on the memory that a container can use
   -c: Used to specify how much percentage of cpu can be used
   -ip : Used to assign a static ip to a container
   -it :Used to open an interactive terminal in the container

13 To see the list of running docker containers
   docker container ls
 
14 To see the list of all containers running and stopped
   docker ps -a

15 To stop a running container
   docker stop container_name/container_id

16 To start a stopped container
   docker start container_name/container_id

17 To restart a container
   docker restart container_name/container_id
   To restart after 10 seconds
   docker restart -t 10 container_name/container_id

18 To delete a stopped container
   docker rm container_name/container_id

19 To delete a running container
   docker rm -f container_name/container_id

20 To stop all running containers
   docker stop $(docker ps -aq)

21 To delete all stopped containers
   docker rm $(docker ps -aq)

22 To delete all container running and stopped
   docker rm -f $(docker ps -aq)

==============================================================================
Day 4
==============================================================================
23 To see the logs of the container
   docker logs  container_name/container_id

24 To see the ports used by the container
   docker port container_name/container_id

25 To come out of a container without exit
   ctrl+p,ctrl+q

26 To go back into the above container
   docker attach container_name/container_id

27 To run any command or process in a container
   docker exec -it container_name/container_id process/command
   Eg: To open bash shell in a container
   docker exec -it container_name/container_id bash

28 To get detailed info about a container
   docker inspect container_name/container_id

Working on docker networks
================================
29 To see the list of docker networks
   docker network ls

30 To create a new netowork
   docker network create --driver network_type  network_name

32 To get detailed info about a network
   docker network inspect network_name/network_id

33 To delete a network
   docker network rm network_name/network_id

34 To connect a running container to a network
   docker network connect network_name/network_id container_name/container_id

35 To remove a running container from a network
   docker network disconnect network_name/network_id container_name/container_id

Working on docker volumes
================================
36 To see the list of docker volumes
   docker volume ls

37 To create a new docker volume
   docker volume create volume_name

38 To delete a docker volume
   docker volume rm volume_name/volume_id

39 To get detailed info about a docker volume
   docker volume inspect volume_name/volume_id

40 To delete all docker volumes
   docker volume prune

==================================================================
UseCase-1
==============
Create a nginx container in detached modes
docker run -p 8888:80  --name webserver -d nginx

To access nginx from the level of browser
public_ip_of_dockerhost:8888

=====================================================================
UseCase-2
====================
Create a jenkins container in detached mode and perform automatic port mapping
docker run --name jenkins -d -P jenkins

To see the port used by the jenkins container
docker port jenkins

To access the jenkins container from the level of browser
public_ip_of_dockerhost:port_no_from_above_command

=============================================================================
UseCase-3
Create a centos container and open interactive terminal in it
docker run --name c1 -it centos

To come out of the container without exit
ctrl+p,ctlr+q

==============================================================================
UseCase-4
Create a mysql container and pass the necessary environment varibles
Login into the db as root user and create few tables

1 Create a mysq container
  docker run --name db -d -e MYSQL_ROOT_PASSWORD=intelliqit mysql

2 To open interactive terminal in the mysql container
  docker exec -it db bash

3 To login as root user
  mysql -u root -p
  Password: intelliqit

4 To see the list of databases
  show databases;

5 To switch into anoy database
   use sys;

6 To create emp and dept tables here
  Open https://justinsomnia.org/2009/04/the-emp-and-dept-tables-for-mysql/
  Copy the script for creating emp and dept tables and paste in mysql container

7 To see the tables data
  select * from emp;
  select * from dept;

==============================================================================
Day 5
==============================================================================
Creating a multi container architecture(microservices architecture)
=====================================================================
Various types of development and testing environments can be
created using docker in the following ways

1  --link  option (depricated)
2  docker-compose
3 Docker networking

4 Python Scripts

--link Option: This is a docker run command option and it is
depricated.

UseCase 1
=============
Create 2 busybox containers and link them

1 Create a busybox container
  docke run  --name c1 -it busybox

2 To come out of the container without exit
  ctrl+p,ctrl+q

3 Create another busybox container and link it with the c1 container
  docker run  --name c1 --link c1:mybusybox -it busybox

4 Check if c2 can ping to c1
  ping c1


UseCase 2
=====================
Create a mysql container and a wordpress container and
link them

1 Create mysql:5 as a container
  docker run  --name db -d -e MYSQL_ROOT_PASSWORD=intelliqit mysql:5

2 Create a wordpress container and link it with a mysql container
  docker run --name mywordpress -d -p 8888:80 --link db:mysql wordpress

3 To check if wordpress is linked with mysql container
  docker inspect mywordpress
  Search for "Links" section

4 To acces the wordpress from browser
  public_ip_of_dockerhost:8888

UseCase 3
================
Create a jenkins container and link with 2 tomcat containers
one for QAserver and another for prodserver

1 Create a jenkins container
  docker run  --name jenkins -d -p 5050:8080 jenkins

2 To access jenkins from browser
  public_ip_of_dockerhost:5050

3 Create a tomcat container as qaserver and link with jenkins
  docker run --name qaserver -d -p 6060:8080 --link jenkins:myjenkins tomcat

4 Create another tomcat container as prodserver and link with jenkins
  docker run --name prodserver -d -p 7070:8080 --link jenkins:myjenkins tomcat

=============================================================================
UseCase 4
===============
Create a postgres container and link it with an adminer container
Note:adminer is a clinet s/w of postgres db which allows us to access
the postgres db from the browser

1 Create a postgres container
  docker run --name mypostgres -d -e POSTGRES_PASSWORD=intelliqit 
                          -e POSTGRES_USER=myuser -e POSTGRES_DB=mydb postgres

2 Create an adminer container and link with postgres container
  docker run --name myadminer -d -P --link mypostgres:db adminer

3 To see the ports used by adminer
  docker port myadminer

4 To access the postgres db from the adminer
  Launch any borwser
  public_ip_of_dockerhost:port_no_from_step3
===================================================================================
  
UseCase 5
===========
Setup LAMP architecture where a mysql container can be linked
with an apache and php container

1 Create a mysql container
  docker run --name db -d -e MYSQL_ROOT_PASSWORD=intelliqit mysql

2 Create an apache and link with mysql container
  docker run --name apache -d -p 9090:80 --link db:mysql  httpd


3 Create a php container and link with apache and mysql containers
  docker run --name php -d --link db:mysql --link apache:httpd php:7.2-apache

4 To check if php container is linked with mysql and apache container
  docker inspect php
  Search for "Links" section

==============================================================================
Day 6
============================================================================
Create a testing environment where a selenium hub container
should be linked with 2 node containers one with chrome
installed and other with firefox installed.The testers should be 
able to run the cross browser,cross platform automation
test scripts

1 Create a selenium hub image
  docker run --name hub -d -p 4444:4444 selenium/hub

2 Create a chrome node and link with the hub container
   docker run --name chrome -d -p 5901:5900 --link hub:selenium  
                                           selenium/node-chrome-debug

3 Create a firefox node and link with the hub container
  docker run --name firefox -d -p 5902:5900 --link hub:selenium 
                                           selenium/node-firefox-debug

4 Check if all 3 containers are running
  docker container ls

5 The above 2 containers are GUI containers and to access the GUI of
  these containers
  a) Install VNC viewer from https://www.realvnc.com/en/connect/download/viewer/
  b) Open vnc viewer
  c) public_ip_of_dockerhost:5901 and 5902
  d) Click on continue--->Enter password:secret


===========================================================================
Docker compose
================
This is another way of creating a mult container architecture.This uses yaml files.The main advantage of docker compose is reusability.

Installing docker compose
=============================
1 Download docker compose
  sudo curl -L "https://github.com/docker/compose/releases/download/1.26.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose

2 Give execute permissions
  sudo chmod +x /usr/local/bin/docker-compose

3 To check the verion of docker compose
  docker-compose --version

URL:https://docs.docker.com/compose/install/

==============================================================================
Create a docker compose file to setup of mysql and wordpress containers

1 vim docker-compose.yml
---
version: '3.8'

services:
 db:
  image: "mysql:5"
  environment:
   MYSQL_ROOT_PASSWORD: intelliqit

 mywordpress:
  image: wordpress
  ports:
   - "8888:80"
  links:
   - "db:mysql"
...

2 To create the services from above file
  docker-compose up -d

3 To see the list of containers
  docker container ls

4 To stop the containers
  docker-compose stop

5 To delete the containers
  docker-compose down

==============================================================================
Create a docker compose file to setup CI-CD environment where
a Jenkins container is linked with 2 tomcat containers

1 vim abc.yml
---
version: '3.8'

services:
 jenkins:
  image: jenkins
  ports:
   - 5050:8080

 qaserver:
  image: tomcat
  ports:
   - 6060:8080
  links:
   - jenkins:myjenkins

 prodserver:
  image: tomcat
  ports:
   - 7070:8080
  links:
   - jenkins:myjenkins
...

2 To setup the services from the above file
  docker-compose -f abc.yml up -d

=============================================================================
Day 7
============================================================================
UseCase 1
==================
Create docker compsoe file to setup the LAMP architecture

vim docker-compose.yml
---
version: '3.8'

services:
 db:
  image: mysql
  environment:
   MYSQL_ROOT_PASSWORD: intelliqit
  container_name: mydb

 apache:
  image: httpd
  ports:
   - 9090:80
  links:
   - db:mysql
  container_name: apache

 php:
  image: php:7.2-apache
  links:
   - db:mysql
   - apache:httpd
  container_name: php
...

========================================================================
UseCase -2
Create a docker compose file to setup the selenium testing environment
vim docker-compose.yml
---
version: '3.8'

services:
 hub:
  image: selenium/hub
  ports:
   - 4444:4444

 chrome:
  image: selenium/node-chrome-debug
  ports:
   - 5901:5900
  links:
   - hub:selenium

 firefox:
  image: selenium/node-firefox-debug
  ports:
   - 5902:5900
  links:
   - hub:selenium
...



==========================================================================
Docker Volumes
=====================
Docker Containers are ephemiral ie temporary but the data processed
by the container should be persistent
To handle this we can use docker volumes,These are used for preserving the 
data even after the container is deleted

Volumes are of 3 types
1 Simple docker volumes
2 Sharable Volumes
3 Docker volume containers


Simple docker volume
----------------------
This is used only for preserving the data even though the 
container is deleted

UseCase
-----------
Create a directory /data on the host machine
Create an ubuntu container and mount this /data
as a volume on this contianer.Create some files in this mounted
volume and check if the files are still available on the host
machine even when the container is deleted

1 Create a directory /data
  mkdir /data

2 Create an ubuntu container and mount /data as a volume
  docker run --name u1 -it -v /data ubuntu

3 In the ubuntu u1 container go into data folder(mounted volume)
  and create some files
  cd data
  touch file1 file2
  exit

4 Indentify the mounted locations
  docker inspect u1
  Go to "Mounts" sections and copy the "Source" path
  
5 Delete the container
  docker rm -f u1

6 Check if the data is still present on host machine
  cd "Source_path_from_step4"
  ls

Sharables Volumes
=======================
These volumes can be shared betwen multiple containers
and the changes done by one container will be reflected
to all other containers

UseCase
===========
Create a folder /data on the dockerhost.mount it as a volume on
centos container c1,later create another centos container c2
and this c2 should use the volume used by c1,create another
centos container c3 and this should use the volume used by c2
Delete all three containers and check if the data is still present 
on the host machine

1 Create /data folder
  mkdir /data

2 Start centos as a container and mount /data as a volume 
  docker run --name c1 -it -v /data centos

3 In the centos c1 container go into the volume and create files
  cd data
  touch file1 file2
  Come out of the container without exit (ctrl+p,ctrl+q)

4 Create another centos container c2 and this container should use
  the volume used by c1
  docker run --name c2 -it --volumes-from c1 centos

5 In the centos c2 container go into the volume and create files
  cd data
  touch file3 file4
  Come out of the container without exit (ctrl+p,ctrl+q)

6 Create another centos container c3 and this container should use
  the volume used by c2
  docker run --name c3 -it --volumes-from c2 centos

7 In the centos c3 container go into the volume and create files
  cd data
  touch file5 file6
  Come out of the container without exit (ctrl+p,ctrl+q)

8 Go into any of the 3 containers and we should see all the files
  docker attach c1 (or) c2 (or) c3
  ls
  exit

9 Identify the mounted location
  docker inspect c1
  Go to "Mounts" section and copy the "Source" path

10 Delete all the 3 containers
   docker rm -f c1 c2 c3

11 Check if the files are still available on the host machine
   cd "source path coped from step 9"
   ls

=========================================================================
Docker volume containers
==============================
These are by directional volumes ie the files from the
host can be accessed in the container and the files from the
container can be accesed on the host

UseCase
----------------
Create a volume "myvolume",Create some files in this volume
and attach it to a centos container.In the centos container
check if this data is available.Similary create some files
in the volume in the container and check if these files are
available on the host

1 Create a docker volume
  docker volume create myvolume

2 Identify the volume location
  docker volume inspect myvolume
  Copy the MountPoint path

3 Go to this mountpoint and create some files
  cd MountPoint+path_from_step2
  touch file1

4 Create a centos container and mount the volume on /tmp folder in the container
  docker run  --name c1 -it -v myvolume:/tmp centos

5 Go into the tmp folder in the container and check if the files from host
  are available
  cd tmp
  ls
  
6  Create few files
   touch file2 file3
   exit

7 Delete the centos container
  docker rm -f c1

8 Check if the data is still present
  cd MountPoint_path_from_step2
  ls

=========================================================================

UseCase 
===============
Create a volume called "newvolume" and store tomcat-users.xml file
Mount this volume into a tomcat container

1 Create a volume
  docker volume create newvolume

2 Identify the Mount location of the volume
  docker volume inspect newvolume

3 Go to the Mount point location
  cd path_of_Mount_point_from_step2

4 Create tomcat-users.xml file
  vim tomcat-user.xml
  <tomcat-users>
     <user username="intelliqit" password="intelliqit" roles="manager-script"/>
  </tomcat-users>

5 Create a tomcat container and mount the above volume
  docker run --name t1 -d -P -v newvolume:/tmp tomcat

6 Go into the shell of the above tomcat container
  docker exec -it t1 bash

7 Check if the tomcat-users.xml file is present in /tmp folder
  cd/tmp
  ls

=======================================================================
Creating customied docker images
======================================
This can be done in 2 ways 
1 Using the docker commit command
2 Using the docker file

Using docker commit command
===================================
UseCase
==========
Create a customsied docker ubuntu image where git and maven are installed

1 Create a ubuntu container
  docker run --name u1 -it ubuntu

2 In this ubuntu container update apt and install git and maven
  apt-get update
  apt-get install -y git 
  apt-get install -y maven
  exit

3 Save the container  u1 as an image
  docker commit u1 myubuntu

4 Delete the ubuntu u1 container
  docker rm -f u1

5 Check if the new image is created
  docker images

6 Create a container from the above created image and check if git and maven
  are already present
  docker run --name u1 -it myubuntu
  git --verion
  mvn --version

=====================================================================
Day 8
=====================================================================
======================================================================
Dockerfile
=================
This is used for creating customised docker images
Dockerfile is a simple text file which uses certain predefined
keywords for perfoming various activites related to image creation.

Important keywords used in Dockerfile
========================================
FROM: This is used to specify the base image from which we should
      create the customised docker image

MAINTAINER: Used to represent the name of the author or the organization
	    that is creating this dockerfile

RUN: This is used to run linux commands in the image,generally it is used
     for upgrading s/w packages and installing s/w applications in the image

CMD: This is used to run an application in the container even when the
     control is outside the container

ENTRYPOINT: Every docker container triggers a specific process when it
            starts and this is the known as the "default process"
            of the container and the the container will be running condition
            only as long as this default process runs,We can specify 
            what should be the default process if a container using this
            option

VOLUME :  Used to attach or mount a deafult volume to a container

EXPOSE: This is used to expose a container port so that it can  be mapped
         with a host port


COPY:  Used to copy files from host to container

ADD: Used to copy files from host to container but it can also download
     files from remote servers into the container

USER: This is used to specify the default user who should login into the
      container

WORKDIR: Used to specify the default directory where the command should be
         executed in a container

SHELL: USed to specify what shell should run in the container Eg: bash,sh,ksh

LABEL:  Used to give a default label to container

STOPSIGNAL:  USed to specify the key sequence that has to be passes to
             stop the container







====================================================
Create a dockerfile from nginx base image and specify the 
maintainer as intelliqit

1 vim dockerfile
FROM nginx
MAINTAINER intelliqit

2 To build an image from the above dockerfile
  docker build -t mynginx .

3 Check if a new image called mynginx is created
  docker images

====================================================
Create a dockerfile from ubuntu base image and install git in it

1 vim dockerfile

FROM ubuntu
MAINTAINER intelliqit
RUN yum -y update
RUN yum install -y git maven

2 To build an image from the above dockerfile
  docker build -t myubuntu .

3 Check if a new image called myubuntu is created
  docker images

4 Create a container from the above image and check if git is installed
  docker run --name c1 -it myubuntu
  git --version
  mvn --version




==================================================================
Cache Busting
===================
Whenever we create an image from a dockerfile docker stores all
the executed instructions in the "dockercache",next time if we
make modifications to the dockerfile and rebuild a new image
docker reads all the previously executed instructions from the '
dockercache and it will execute only the new instructions
This is a time saving machanism provided by docker

Eg:
FROM ubuntu
RUN apt-get update
RUN apt-get install -y git

If we create an image from the above dockerfile it save all these instructions
in the dockecache and alter if we add the below statement
RUN apt-get install -y tree
and if we build an image from this docker file it will execute on the 
latest instruction

The disadvantage is if we edit the docker file after a huge timegap
then we can end up installing s/w from a reposiotry that was updated 
log time back

To overcome this we can use "cache busting" ie we can tell docker
to build an image from the dockerfile without reading previosuly
executed instructions from the dockercache

docker build --no-cache -t myubuntu .

===========================================================================

Create a dockerfile from ubuntu base image and install 
ansible in it

1 vim dockerfile
  FROM ubuntu
  MAINTAINER intelliqit
  RUN apt-get update
  RUN apt-get install -y software-properties-common
  RUN apt-get update
  RUN apt-get install -y ansible

2 Create an image from the above file
  docker build -t ubuntu1 .

3 Create a container from the newly created image and check for ansible
  docker run  --name u1 -it ubuntu1
  ansible --version

==============================================================================
Create a dockerfile from  base centos image and mount /data
as the default volume

1 vim dockerfile
FROM centos
MAINTAINER intelliqit
VOLUME /data

2 Create an image from the above dockerfile
  docker build -t mycentos .

3 Create a container from the above image
  docker run --name c1 -it mycentos

4 Go into the mounted volume and create few files
  cd data
  touch file1 file2
  exit

5 Check the mounted location
  docker inspect u1
  Go to "Mounts" section and copy the "Source" path

6 Delete the container
  docker rm -f c1

7 Check if the data is still present on the host machine
  cd "Source_path_from_step5"
  ls

==========================================================================
Day 9
==========================================================================
Create a dockerfile from nginx base image and expose 90 as
the container port

1 vim dockerfile
FROM nginx
MAINTAINER intelliqit
EXPOSE 90

2 To build an image from the above dockerfile
  docker build -t mynginx .

3 Create a container from the above image
  docker run --name n1 -d -P mynginx

4 Check the port of the container
  docker port n1

=======================================================================

Create a dockerfile from jenkins base image and make the default user
as root and also install git amd maven

1 vim dockerfile

FROM jenkins
MAINTAINER intelliqit
USER  root

RUN apt-get update
RUN apt-get install -y git maven

2 Create an image from the above file
  docker build -t myjenkins .

3 Create a container from the above image
  docker run --name j1 -d -P myjenkins

4 Go into the bash shell of the container and check who is the 
  default user and also check if the git and maven are present
  docker exec -it j1 bash
  whoami
  git --version
  mvn --version





===========================================================================
Create a dockerfile from ubuntu base image and download
jenkins.war into it

1 vim dockerfile
FROM ubuntu
MAINTAINER intelliqit
ADD http://mirrors.jenkins.io/war-stable/2.235.5/jenkins.war  /

2 Create an image from the above file
  docker build -t myubuntu .

3 Create a container from the above image and we will see jenkins.war in it
  docker run --name u1 -it myubuntu

  ls
========================================================================
Create a dockerfile from ubuntu dockerimage and install java in
it and download jenkins.war.
When we start the contaienr it shoudl execute
java -jar jenkins.war as the default process

1 vim dockerfile
FROM ubuntu
MAINTAINER intelliqit
RUN apt-get update
RUN apt-get install -y openjdk-8-jdk
ADD http://mirrors.jenkins.io/war-stable/latest/jenkins.war /
ENTRYPOINT ["java","-jar","jenkins.war"]
EXPOSE 8080

2 Create an image from the dockerfile
  docker build -t myubuntu .

3 Create a container and check if jenkins is running
  docker run  --name u1 -it myubuntu
  This will generate the logs of jenkins

4 To access the jenkins from the brpwser
  public_ip_of_dockerhost:8080

============================================================================
Day 10
============================================================================
Create a dockerfile from ubuntu image and make it behave
like an nginx container

1 vim dockerfile
FROM ubuntu
MAINTAINER intelliqit
RUN apt-get update
RUN apt-get install -y nginx
ENTRYPOINT ["/usr/sbin/nginx","-g","daemon off;"]
EXPOSE 80

2 Create an image from the above dockerfile
  docker build -t myubuntu .

3 Create a container a from the above image
  docker run --name c1 -d -P myubuntu

4 to access the nginx from browser
  public_ip_dockerhost:port_no_from_step3

====================================================================
Create a dockerfile from centos base image and install
httpd in it.Copy index.html into this and make httpd as the
default process of the container

1 vim index.html
<html>
 <body>
         <h1>Welcome to IntelliQIt</h1>
  </body>
</html>

2 vim dockerfile
FROM centos
MAINTAINER intelliqit
RUN yum -y update
RUN yum install -y httpd 
COPY index.html /var/www/html
ENTRYPOINT ["/usr/sbin/httpd","-D","FOREGROUND"]
EXPOSE 80

3 Create an image from the above file
  docker build -t mycentos .

4 Create a container from the above image
  docker run --name c1 -d -P mycentos

5 Check if we can access this container from browser
  public_ip_dockerhost:port_no_from_step4

=========================================================================
========================================================================

Docker Networking
=====================
Docker uses 4 types os networks
1 Bridge: This is the deafult network of docker when contianers are
          running on a single docker host

2 Host: This is used when we want to run a single container on a dockerhost
         and this contianer communicates only with the host machine

3 Null: This is used for creating isolated containers ie these containers
        cannot communicate with th host machine or with other containers

4 Overlay: This is used when containers are running in a distributed environment
           on multiple linux servers



UseCase
===============
Create 2 bridge networks intelliq1 and intelliq2
Create 2 busybox containers c1,c2 and c3
c1 and c2 should run on intelliq1 network and shoul ping each other
c3 should run on intelliq2 network and it should not be able to ping c1 or c2
Now put c2 on intelliq2 network,since c2 is on both intelliq1 and intelliq2
networks it should be able to ping to both c1 and c3
but c1 and c3 should not ping each other directly

1 Create 2 bridge networks
  docker network create --driver bridge intelliq1
  docker network create --driver bridge intelliq2

2 Check the list of available networks
  docker network ls

3 Create a busybox container c1 on intelliqi1 network
  docker run --name c1 -it --network intelliq1 busybox
  Come out of the c1 container without exit ctrl+p,ctrl+q

4 Identify the ipaddress of c1
  docker inspect c1

5 Create another busybox container c2 on intelliq1 network
  docker run --name c2 -it --network intelliq1 busybox
  ping ipaddress_of_c1    (It will ping)
  Come out of the c2 container without exit ctrl+p,ctrl+q

6 Identify the ipaddress of c2
  docker inspect c2

7 Create another busybox container c3 on intelliq2 network
  docker run --name c3 -it --network intelliq2 busybox
  ping ipaddress_of_c1  (It should not ping)
  ping ipaddress_of_c2  (It should not ping)
  Come out of the c3 container without exit ctrl+p,ctrl+q

8 Identify the ipaddress of c3
  docker inspect c3 

9 Now attach intelliq2 network to c2 container
  docker network connect intelliq2 c2

10 Since c2 is now on both intelliq1 and intelliq2 networks it should ping
   to both c1 and c3 containers
   docker attach c2
   ping ipaddress_of_c1  (It should  ping)
   ping ipaddress_of_c3  (It should  ping)
   Come out of the c2 container without exit ctrl+p,ctrl+q

11 But c1 and c3 should not ping each other
   docker attach c3
   ping ipaddress_of_c1  (It should not ping)

======================================================================
Day 11
=====================================================================
Create docker compose file to start a postgres db and adminer
webapplication on the network that we created

1 Create a new network
  docker network create --driver bridge --subnet=192.168.2.0/24 new_intelliqit

2 Create a dcoker compose file
vim docker-compose.yml
---
version: '3.8'

services:
 db:
  image: postgres
  environment:
   POSTGRES_PASSWORD: intelliqit
   POSTGRES_USER: user1
   POSTGRES_DB: mydb

 adminer:
  image: adminer
  ports:
   - 8888:8080

networks:
 default:
  external:
   name: new_intelliqit

3 To create services from the above dockerfile
  docker compose up -d

4 Check if the adminer can access the db from browser
  public_ip_of_dockerhost:8888

5 Check the ipaddress of the containers 
  docker container ls
  docker insepct container_id_from_above_command

============================================================================
UseCase
Create a docker compose file to link mysql and wordpress
containers also this file should create 2 volumes one for
each container and it should create 2 bridge networks
one for each container

vim docker-compose.yml
---
version: '3.8'

services:
 db:
  image: mysql:5
  environment:
   MYSQL_ROOT_PASSWORD: intelliqit
  volumes:
   - db:/var/lib/mysql
  networks:
   - abc

 wordpress:
  image: wordpress
  ports:
   - 8989:80
  volumes:
   - wordpress:/var/www/html
  networks:
   - xyz

volumes:
 wordpress:
 db:

networks:
 abc: {}
 xyz: {}


2 To create container from the above docker file
  docker-compose up -d

3 Check if 2 networks abc,xyz are created
  docker network ls

4 Check if 2 volumes  db and wordpress are created
  docker volume ls

===================================================================
Create a dockerfile to create a customised docker image 
and call that dockerfile in docker-compose file

vim dockerfile

FROM jenkins
MAINTAINER intelliqit
USER root
RUN apt-get update
RUN apt-get install -y git maven

vim docker-compose.yml

---
version: '3.8'

services:
 jenkins:
  build: .
  ports:
   - 5050:8080

 qaserver:
  image: tomcat
  ports:
   - 6060:8080
  

 prodserver:
  image: tomcat
  ports:
   - 7070:8080
  
...

===========================================================================
Day 12
==========================================================================
Container Orchestration
===========================
This is the process of running docker containers in a distributed
environment on multiple linux servers

Advantages
=================
1 Load Balancing: The services can be deployed on multiple containers
running on multiple servers so that the load can be distributed between
containers and servers.On all these containers we can have only one
service running

2 Scalling: Depending on the business requirement we can increase or
decrease the number of containers on which a specific service is running
without the end users experiencing any downtime

3 Rolling update: Services running on docker in a production environment
can be upgraded to a higher version or rolled back to a lower version
without the end users experiencing downtime.This is done by upgrading
one container after other in a rolling manner

4 High Availability/Disaster Recovery: If a container fails or the server
on which these containers are running crashes still we can maintian
the "desired number" of containers on the remaining servers.

Popular tool for container orchestration
===============================================
1 Docker Swarm
2 Kubernetes
3 OpenShift
4 Apache Mesos


Docker Swarm
==============
This is a contianer orchestration tool of docker and it helps us to
managed docker containers running on multiple servers

The main machine where docker swarm is initilised is called as MAanger
The remianing machines that take the work load are called as Workers.



Setup of Docker Swarm
=======================
1 Create few AWS instances and name as Manager,Worker1 etc

2 Install docker in all of them

3 Change the hostname
  vim /etc/hostname
  Remove the content and replace it with Manager (or) Worker1 (or) Worker2

4 To initilise the swarm manager
  a) Connect to Manager AWS instance uisng git bash
  b) docker swarm init 
     This command will create a docker swarm and it will genrate a token id
     as output,We should this token id in the Worker machine and then they will
     join swarm as workers
  c) Connect to Workers and execute the token id of 4-b step

=============================================================================

Note: Each machine used in swarm is called as node and the collection
of all these nodes is called as Swarm cluster.

Ports used by swarm
-------------------------------
TCP port 2376 for secure Docker client communication. This port is required for Docker Machine to work. Docker Machine is used to orchestrate Docker hosts.

TCP port 2377. This port is used for communication between the nodes of a Docker Swarm or cluster. It only needs to be opened on manager nodes.

TCP and UDP port 7946 for communication among nodes (container network discovery).
UDP port 4789 for overlay network traffic (container ingress networking).

=============================================================================
Day 13
=============================================================================
LoadBalancing:
-------------------
UseCase-1
Create nginx with 5 replicas in docker swarm cluster
docker service create --name webserver -p 8989:80 --replicas 5 nginx

To see on which node these replicas are running
docker service ps webserver

To access nginx from the level of browser
public_ip_of_manager/Worker1/Worker2:8989






============================================================================
UseCase-2
Create mysql with 3 replicas
docker service create --replicas 3 --name db 
                          -e MYSQL_ROOT_PASSWORD=intelliqit mysql:5

To check where these replicas are running
docker service ps db

To delete the mysql db service
docker service rm db

To see the list of services available
docker service ls
=========================================================================


Scalling
=============
UseCase 1
Create tomcat with 3 replicas and increase it to 8 
later scale down to 2

1 Create tomcat with 3 replicas
  docker service create  --name appserver -p 9999:8080 --replicas 3 tomcat

2 To check if 3 replicas of tomcat are running
  docker service ps appserver

3 To scale the replicas count from 3 to 8
  docker service scale appserver=8

4 Check if 8 replicas of tomcat are running
  docker service ps appserver

5 To scale the replicas count to 2
  docker service scale appserver=2

6 Check if 2 replicas are now running
  docker service ps appserver

==========================================================================
Day 14

==========================================================================
Rolling updates
=====================
Create redis:3 with 5 replicas and update it to redis:4
later roll back to redis:3

1 Create redis:3 with 5 replicas
  docker service create --name myredis --replicas 5  redis:3

2 Check if 5 replicas of redis:3 are running
  docker service ps myredis

3 Perform a rolling update from redis:3 to redis:4
  docker service update --image redis:4 myredis

4 Check if 5 replicas of redis:3 are shut down and redis:4 are running
  docker service ps myredis

5 Perform a roll back from redis:4 to redis:3
  docker service update --rollback myredis

6 Check if redis:4 is shutdown and redis:3 is running
  docker service ps myredis



=========================================================================
1 To remove a node from swarm via Manager
  docker node update --availability drain node_name 
  Eg: For Worker1 to leave the swarm
  docker node update --availability drain Worker1

2 To make Worker1 rejoin docker swarm
  docker node update --availability active Worker1

3 Workers can also leave swarm
  a) Connect to Worker2 using git bash
     docker swarm leave
  b) On Manager check the status of nodes
     docker node ls
     It will show worker2 as "Down"
  c) To remove this Worker2 from the cluster
     docker node rm Worker2

4 Manager cam leave swarm
  docker swarm leave --force

5 To generate the token id to a machine to join swarm as worker
  docker swarm join-token worker

6 To generate the token id to a machine to join swarm as manager
  docker swarm join-token manager

7 To promote Worker1 as manager
  docker node promote Worker1

8 To demote a Worker1 from manager to worker
  docker node demote Worker1






======================================================================
FailOver Scenarios of Workers
================================
Create httpd with 6 replicas and delete one replica running on the manager
Check if all 6 replicas are still running

Drain Worker1 from the docker swarm and check if all 6 replicas are running
on Manager and Worker2,make Worker1 rejoin the swarm

Make Worker2 leave the swarm and check if all the 6 replicas are
running on Manager and Worker1

1 Create httpd with 6 replicas
  docker service create  --name webserver -p 9090:80 --replicas 6 httpd

2 Check the replicas running on Manager
  docker service ps webserver | grep Manager

3 Check the container id
  docker container ls

4 Delete a replica
  docker rm -f container_id_from_step3

5 Check if all 6 replicas are running
  docker service ps webserver

6 Drain Worker1 from the swarm
  docker node update --availability drain Worker1

7 Check if all 6 replicas are still running on Manager and Worker2
  docker service ps webserver

8 Make Worker1 rejoin the swarm
  docker node update --availability active Worker1

9 Make Worker2 leave the swarm
  Connect to Worker2 using git bash
  docker swarm leave
  Connect to Manager
  
10 Check if all 6 replicas are still running
   docker service ps webserver

===========================================================================
Day 15
============================================================================
======================================================================
FailOver Scenarios of Managers
====================================
If a worker instance crashses all the replicas running on that
worker will be moved to the Manager or the other workers.
If the Manager itself crashes the swarm becomes headless 
ie we cannot perfrom container orchestration activites in this
swamr cluster

To avoid this we should maintain multiple managers
Manager nodes have the status as Leader or Reachable

If one manager node goes down other manager becomes the Leader
Quorum is resonsible for doing this activity and if uses a RAFT
algorithm for handling the failovers of managers.Quorum also 
is responsible for mainting the min number of manager

Min count of manager required for docker swarm should be always
more than half of the total count of Managers

Total Manager Count  -    Min Manager Required
      1              -           1
      2              -           2
      3              -           2
      4              -           3
      5              -           3
      6              -           4
      7              -           4
==============================================================================
Day 21
==============================================================================
Overlay network
==================
This is the deafult network used by docker swarm
and it perfroms network load balancing
ie even if donot have a replica running on a specific node
still we will be able to access that replica service via that node

UseCase
=============
Create 2 custom overlay networks intelliq1 intelliq2
Create htttpd as a service in swarm on the intelliq1 network
Create tomcat as a service in swarm on the default overlay (ingres) 
network and later perform a rolling network update to intelliq2 network

1 Create 2 overlay networks
  docker network create --driver overlay intelliq1
  docker network create --driver overlay intelliq2

2 Check if 2 new networks are create
  docker network ls

3 Start httpd with 5 replicas on intelliq1 network
  docker service create --name webserver -p 8888:80 --replicas 5
                                           --network intelliq1 httpd


4 Check if httpd is running on intelliq1 network
  docker service inspect webserver
  This command generates the output in JSON file format 
  To get the above output in simple text format
  docker service inspect webserver  --pretty

5 Start tomcat with 5 replcias on the defult ingres network
  docker service create --name appserver -p 9090:8080 --replicas 5 tomcat

6 Perfrom a rolling network update to intelliq2 network
  docker service update --network-add intelliq2 appserver

7 Check if tomcat in now running on intelliq2 network
  docker service inspect appserver  --pretty

Note: To remove a service from a  network
  docker service update --network-rm network_name service_name


=================================================================================
Day 16
==================================================================================
================================================================================
Docker Stack
====================
This is used for creating a multi container architecture using
docker compose and deploy it in the swarm cluster

docker compose + swarm = docker stack
docker compose + kubernetes = kompose

1 To see the list of stacks
  docker stack ls

2 To create a stack
  docker stack deploy -c stack_filename/docker_compose_file  stack_name

3 To see the list of nodes where the stack services are running
  docker stack ps stack_name

4 To see the list of services in a stack
  docker stack service stack_name

5 To delete a stack
  docker stack rm stack_name







============================================================================
Create a docker stack file for wordpress and mysql
1 vim stack1.yml
---
version: '3'

services:
 mydb:
  image: mysql:5
  environment:
   MYSQL_ROOT_PASSWORD: intelliq

 wordpress:
  image: wordpress
  ports:
   - 8080:80
  deploy:
   replicas: 3
...

2 To deploy this stack service
  docker stack deploy -c stack1.yml wordpress

3 To see where the  stack replicas are running
  docker stack ps wordpress

4 To remove the entire stack
  docker stack rm wordpress


Create a stack file where 2 replicas of jenkins
3 replicas of tomcat as qaserver and 4 replicas of tomcat
as prodserver,
jenkins replicas should run only on manager
tomcat qaserver replicas only on worker1
tomcat prodserver replicas only on worker2

vim stack2.yml
services:
 jenkins:
  image: jenkins
  ports:
   - 5050:8080
  deploy:
   replicas: 2
   placement:
    constraints:
     - node.hostname == Manager

 qaserver:
  image: tomcat
  ports:
   - 6060:8080
  deploy:
   replicas: 3
   placement:
    constraints:
     - node.hostname == Worker1

 prodserver:
  image: tomcat
  ports:
   - 7070:8080
  deploy:
   replicas: 4
   placement:
    constraints:
     - node.hostname == Worker2
    

To deploy the services in swarm
docker stack deploy -c stack2.yml ci-cd

To check if all the serivces are deployed accoring to constraints
docker stack ps ci-cd

To delete the stack
docker stack rm ci-cd




UseCase
===============
Create a docker stack file to setup the selenium testing environment
and also put an upper limit on the h/w allocation

1 vim stack3.yml
---
version: '3'

services:
 hub:
  image: selenium/hub
  ports:
   - 4444:4444
  deploy:
   replicas: 1
   resources:
    limits:
     cpus: "0.1"
     memory: "200M"

 chrome:
  image: selenium/node-chrome-debug
  ports:
   - 5901:5900
  deploy:
   replicas: 2
   resources:
    limits:
     cpus: "0.01"
     memory: "50M"

 firefox:
  image: selenium/node-firefox-debug
  ports:
   - 5902:5900
  deploy:
   replicas: 2
   resources:
    limits:
     cpus: "0.01"
     memory: "100M"

2 To deploy the the services from the above stack file
  docker stack deploy -c stack3.yml selenium


====================================================================

=====================================================================


Docker Secrets
==================
This is  a feature of docker swarm using which we can pass encrypted
data to replicas running in swarm cluster
These secrets are creatred on the host machine and they can be accessed
from the replicas but the content cannot be modified in
the replicas

1 To create a secret and pass some data into it
  echo "Hello Intelliq" | docker secret create mysecret -

2 Create an aline:redis with 5 replicas and make it access the secret data
  docker service create --name myredis --replicas 5 --secret mysecret redis

3 Capture the container id
  docker container ls

4 Check if the secret data is available in the container
  docker exec -it container_id_from_step4  cat /run/secrets/mysecret

==========================================================================
UseCase
--------------
Create 3 secrets for postgres user,password and db
and pass them to the stack file

1 Create secrets
  echo "intelliqit" | docker secret create pg_password -
  echo "myuser" | docker secret create pg_user -
  echo "mydb" | docker secret create pg_db -

2 Check if the secrets are created
  docker secret ls

3 Create the docker stack file to work on these secrets
  vim stack4.yml
---
version: '3.1'
services:
  db:
    image: postgres
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/pg_password
      POSTGRES_USER_FILE: /run/secrets/pg_user
      POSTGRES_DB_FILE: /run/secrets/pg_db
    secrets:
     - pg_password
     - pg_user
     - pg_db

  adminer:
    image: adminer
    restart: always
    ports:
      - 8080:8080
    deploy:
     replicas: 2

secrets:
    pg_password:
     external: true
    pg_user:
     external: true
    pg_db:
     external: true

...

To deploy the stack file
docker stack deploy -c stack4.yml

========================================================================
Day 17
=======================================================================
Kubernetes
===========================
================================================================================
Kubernetes
======================

Menions: This is an individual node used in kubernetes
Combination of these minions is called as Kubernetes cluster

Master is the main machine which triggers the container orchestraion
It distributes the work load to the Slaves

Slaves are the nodes that accept the work load from the master
and handle activites load balancing,autoscalling,high availability etc

kubeadm: This is an application that is responible for creating the 
Master node and it also stores info about the salves

kubeapi: This is an application that runs on the salves and it it
accepts the instructions from kubeadm and executes them on the slaves

kubectl: This is an application that triggers the kubernetes commands

Kubernetes uses various of types of Object

1 Pod: This is a layer of abstraction on top of a container.This is the samallest
  object that kubernetes can work on.In the Pod we have a container.
  The advantage of using a Pod is that kubectl commands will work on the Pod and the 
  Pod communicates these instructions to the container.In this way we can use the
  same  kubectl irresepective of which technology containers are in the Pod.


2 Service: This is used for port mapping and network load balancing

3 NameSpace: This is used for creating partitions in the cluster.Pods running
 in a namespace cannot communicate with other pods running in other namespace

4 Secrets: This is used for passing encrypted data to the Pods

5 ReplicationController: This is used for managing multiple replicas of PODs
and also perfroming saclling 

6 ReplicaSet: This is similar to replicationcontroller but it is more advanced
where features like selector can be implemented

7 Deployment: This used for perfroming all activites that a Replicaset can do
  it can also handle rolling update



========================================================================
Setup of Kubernetes
===============================
Free
===========
1 http://katakoda.com
(or)
2 http://playwithk8s.com

Paid
==============
1 Signup for a Google cloud account
2 Click on Menu icon on top right corner--->Click on Kubernetes Engine-->Clusters
3 Click on Create cluster--->Click on Create=
==============================================================================

UseCase 1
Create an nginx pod in Kubernetes cluster and later
delete it

kubectl run --image nginx webserver 

To see the list if pods
kubectl get pods

To see the list if pods along with their ip address and node where they are running
kubectl get pods -o wide

To see detailed info about the pod
kubectl describe pods pod_name
Eg:  kubectl describe pods webserver

To delete the pod
kubectl delete pods webserver

=============================================================================
UseCase
Create a mysql pod and also pass the necessary environment varibles
kubectl run --image mysql:5 db --env MYSQL_ROOT_PASSWORD=intelliqit

To see the list of pods
kubectl get pods

To open interactive terminal in this pod
kubeclt exec -it db -- bash

To login as mysql root user
mysql -u root -p
Password: intelliqit

To see the list of databases
show databases;

To use a specific database
use db;

To create emp and dept tables here
Open https://justinsomnia.org/2009/04/the-emp-and-dept-tables-for-mysql/
Copy code and paste in mysql pod

To see the list of tables
select * from emp;
select * from dept;


To delete the pods
kubectl delete pods db

========================================================================

Kubernetes Definition file
=================================
Kubernetes performs container orchestration uisng certain definition
file.These files are created using yml and they have 4 top level
fields

apiVersion:
kind:
metadata:
spec:

apiVersion: Every kubernetes object uses a specific Kubernetes code
library that is called apiVersion.Only once this code library is imported
we can start working on specific objects

kind: This represents the type of Kubernetes object that we want to us
      eg: Pod,Replicaset,Service etc

metadata: Here we give a name to the Kubernetes object and also some
          labels.These labels can be used later for performing group
          activites

spec: This is where we store info about the exact docker image,container name
      environment varibales,port mapping etc


Kind              apiVersion
=================================
Pod               v1
Service           v1
NameSpace         v1
Secrets           v1
ReplicationController v1
ReplicaSet        apps/v1
Deployment        apps/v1

=========================================================================
Day 18
==========================================================================
UseCase-1
Create a pod definition file to start an nginx in a pod 
name the pod as nginx-pod,name the container as appserver

vim pod-defintion1.yml

---
apiVersion: v1
kind: Pod
metadata:
 name: nginx-pod
 labels:
  author: intellqit
  type: proxy
spec:
 containers:
  - name: appserver
    image: nginx
...

To create a pod from the above file
kubectl create -f pod-defintion1.yml

To see the list of pods
kubectl get pods

To see the pods along with the ipaddress and name of the slave where it is running
kubectl get pods -o wide

============================================================================


To delete the pods created from the above file
kubectl delete -f pod-definition1.yml


Create a pod defintion file to start a postgres container
Name of the container should be mydb,pass the necssary environment
variables,this container should run in a pod called postgres-pod
and give the labels as author=intelliqit and type=database


vim pod-definition2.yml
---
apiVersion: v1
kind: Pod
metadata:
 name: postgres-pod
 labels:
  author: intelliqit
  type: database
spec:
 containers:
  - name: mydb
    image: postgres
    env:
     - name: POSTGRES_PASSWORD
       value: myintelliqit
     - name: POSTGRES_USER
       value: myuser
     - name: POSTGRES_DB
       value: mydb
...

To create pods from the above defintion file
kubectl create -f pod-defintion2.yml

To delete the pods
kubectl delete -f pod-definition2.yml


Create a pod defintion file to start a jenkins container in a pod
called jenkins-pod,also perform port mapping to access the jenkins
from a browser

vim pod-definition3.yml
---
apiVersion: v1
kind: Pod
metadata:
 name: jenkins-pod
 labels:
  author: intelliqit
  type: ci-cd
spec:
 containers:
  - name: myjenkins
    image: jenkins
    ports:
     - containerPort: 8080
       hostPort: 8080
...

To create pods from the above file
kubectl create -f pod-defintion3.yml

To see the list of pods along with nodes where they are running
kubectl get nodes -o wide

To get the external ip of the node
kubectl get node -o wide

To access then jenkins from browser
external_ip_of_slavenode:8080


========================================================================
ReplicationController
=======================
This is a high level Kubernets object that can be used for handling 
multiple replicas of a Pod.Here we can perfrom Load Balancing
and Scalling

ReplicationController uses keys like "replicas,template" etc in the "spec" section
In the template section we can give metadata related to the pod and also use
another spec section where we can give containers information

Create a replication controller for creating 3 replicas of httpd
vim repilication-controller.yml
---
apiVersion: v1
kind: ReplicationController
metadata:
 name: httpd-rc
 labels:
  author: intelliqit
spec:
 replicas: 3
 template:
  metadata:
   name: httpd-pod
   labels:
    author: intelliqit
  spec:
   containers:
    - name: myhttpd
      image: httpd
      ports:
       - containerPort: 80
         hostPort: 8080
...

To create the httpd replicas from the above file
kubectl create -f replication-controller.yml

To check if 3 pods are running an on whcih slaves they are running
kubectl get pods -o wide

To delete the replicas
kubectl delete -f replication-controller.yml



ReplicaSet
===================
This is also similar to ReplicationController but it is more
advanced and it can also handle load balancing and scalling
It has an additional field in spec section called as "selector"
This selector uses a child element "matchLabels" where the
it will search for Pod based on a specific label name and try to add
them to the cluster

=======================================================================
Day 19
=======================================================================
Create a replicaset file to start 4 tomcat replicas  and then perform scalling
vim replica-set.yml
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
 name: tomcat-rs
 labels:
  type: webserver
  author: intelliqit
spec:
 replicas: 4
 selector:
  matchLabels:
   type: webserver
   
 template:
  metadata:
   name: tomcat-pod
   labels:
    type: webserver
  spec:
   containers:
    - name: mywebserver
      image: tomcat
      ports:
       - containerPort: 8080
         hostPort: 9090

To create the pods from the above file
kubectl create -f replica-set.yml

Scalling can be done in 2 ways
a) Update the file and later scale it

b) Scale from the coomand prompt withbout updating the defintion file

a) Update the file and later scale it
  Open the replicas-set.yml file and increase the replicas count from 4 to 6
  kubectl replace -f replicas-set.yml
  Check if 6 pods of tomcat are running
  kubectl get pods

b) Scale from the coomand prompt withbout updating the defintion file
   kubectl scale --replicas=2 -f replica-set.yml
================================================================

Deployment
================

This is also a high level Kubernetes object which can be used for
scalling and load balancing and it can also perfrom rolling update

Create a deployment file to run nginx:1.7.9 with 3 replicas
Later perform a rolling update to nginx:1.9.1

vim deployment1.yml
---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: nginx-deployment
 labels:
  author: intelliqit
  type: proxyserver
spec:
 replicas: 3
 selector:
  matchLabels:
   type: proxyserver
 template:
  metadata:
   name: nginx-pod
   labels:
    type: proxyserver
  spec:
   containers:
    - name: nginx
      image: nginx:1.7.9
      ports:
       - containerPort: 80
         hostPort: 8888
 
To create the deployment from the above file
kubectl create -f deployment.yml

To check if the deployment is running
kubectl get deployment

To see if all 3 pod of nginx are running
kubectl get pod

Check the version of nginx
kubectl describe pods nginx-deployment | less       (nginx:1.7.9)

Perform a rolling update to nginx:1.9.1
kubectl --record deployment.apps/nginx-deployment set image                           deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1

To check if the update the has happened
kubectl describe pods nginx-deployment | less      (nginx:1.9.1)


==============================================================================
Kompose
================
This is used to implement docker compose to create a multi
container architecture in Kubernetes

Implementing docker compose can be done using Kompose
docker compose + docker swarm = docker stack
docker compose + Kubernetes = Kompose

Setup
===========
1 Download Kompose
  curl -L https://github.com/kubernetes/kompose/releases/download/v1.18.0/kompose-linux-amd64 -o kompose

2 Give execute permissions
  chmod +x kompose

3 Move it to PATH
  sudo mv ./kompose /usr/local/bin/kompose

4 To check if the installion is successfull
  kompose version

Digital Ocean URL
========================
https://www.digitalocean.com/community/tutorials/how-to-migrate-a-docker-compose-workflow-to-kubernetes

vim docker-compose.yml
---
version: '3'
services:
 db:
  image: mysql:5
  environment:
   MYSQL_ROOT_PASSWORD: intelliqit

 wordpress:
  image: wordpress
  ports:
   - 5050:80
  deploy:
   replicas: 3

To start the services 
kompose up

To see the list of Pods
kubectl get pods

To see the list of all the kubernetes object created
kubectl get all

To convert the docker compose file into kubernetes 
kompose convert

To remove the services
kompose down

==========================================================================
Day 20
==========================================================================
vim docker-compose.yml
---
version: '3'
services:
 db:
  image: mysql:5
  environment:
   MYSQL_ROOT_PASSWORD: intelliqit

 wordpress:
  image: wordpress
  ports:
   - 5050:80
  deploy:
   replicas: 3

To start the services 
kompose up

To see the list of Pods
kubectl get pods

To see the list of all the kubernetes object created
kubectl get all

To convert the docker compose file into kubernetes 
kompose convert

To remove the services
kompose down

============================================================================


Namespace in kubernetes
==========================
Namespaces are used to create partitions in the Kubernetes cluster
Pods runnign in different namespaces cannot communicate with
each other

To create Namespaces
===========
vim namespace.yml
---
apiVersion: v1
  kind: Namespace
  metadata:
    name: test-ns
...

kubectl apply -f namespace.yaml 

To see the list of namespace
================================
kubectl get namespace

Create a pod on that namespace
===================================
vim pod-definition4.yml

---
apiVersion: v1
kind: Pod
metadata:
 name: jdk-pod
 namespace: test-ns
 labels:
  author: intelliqit
spec:
 containers:
  - name: java
    image: openjdk:12
...

To see list of pods in a namespace
======================================
kubectl get pods -n test-ns

To delete a namespace
===========================
kubectl delete namespace test-ns


============================================================================
Volumes
==================
---
apiVersion: v1
kind: Pod
metadata:
 name: redis-pod
 labels:
  author: intelliqit
spec:
 containers:
  - name: redis
    image: redis
    volumeMounts:
     - name: redis-volume
       mountPath: /data/redis
 volumes:
  - name: redis-volume
    emptyDir: {}

Create a pod from the above file
kubectl create -f volumes.yml

To check if the volume is mounted
kubectl exec -it redis-pod -- bash

Go to the redis folder and create some files
cd redis
cat > file
Store some data in this file

To kill the redis pod install procps
apt-get update
apt-get install -y procps

Identify the process id of redis
ps aux
kill 1

Check if the redis-pod is recreated
kubectl get pods
We will see the restart count changes for this pod

If we go into this pods interactive terminal
kubectl exec -it redis-pod -- bash

We will see the data but not the s/w's (procps) we installed
cd redis
ls

ps  This will not work

========================================================================
Secrets
========================================================================
This is used to send encrypted data to the definiton files
Generally passwords for Databases can be encrypted using this

Create a secret file to store the mysql password
vim secret.yml
---
apiVersion: v1
kind: Secret
metadata:
 name: mysql-pass
type: Opaque
stringData:
 password: intelliqit

...

To deploy the secret
kubectl create -f secret.yml

Create a pod defintion file to start a mysql pod and pass the environment
varible using the above secret
vim pod-defitintion5.yml
---
apiVersion: v1
kind: Pod
metadata:
 name: mysql-pod
 labels:
  author: intelliqit
  type: db
spec:
 containers:
  - name: mydb
    image: mysql:5
    env:
     - name: MYSQL_ROOT_PASSWORD
       valueFrom:
        secretKeyRef:
         name: mysql-pass
         key: password
...

To create pods from above file
kubect create -f pod-defintion5.yml

==============================================================================
Create secrets for postgres password
vim secret2.yml
---
apiVersion: v1
kind: Secret
metadata:
 name: postgres-secrets
type: Opaque
stringData:
 password: intelliqit
 user: myuser
 db: mydb 

To create a secret from the above file
kubectl create -f secret2.yml

Create a pod defitnition file that start starts postgres db using the above secrets
---
apiVersion: v1
kind: Pod
metadata:
 name: postgres-pod
 labels:
  author: intelliqit
  type: database
spec:
 containers:
  - name: mydb
    image: postgres
    env:
     - name: POSTGRES_PASSWORD
       valueFrom:
        secretKeyRef:
         name: postgres-secrets
         key: password
     - name: POSTGRES_USER
       valueFrom:
        secretKeyRef:
         name: postgres-secrets
         key: 
     - name: POSTGRES_DB
       value: mydb

=======================================================================
Day 21
=======================================================================
Use Case
=================
Create a service defintion file for port mapping an nginx pod

vim pod-defintion1.yml
---
apiVersion: v1
kind: Pod
metadata:
 name: nginx-pod
 labels:
  author: intellqit
  type: proxy
spec:
 containers:
  - name: appserver
    image: nginx
=========================================================
vim service1.yml
---
apiVersion: v1
kind: Service
metadata:
 name: nginx-service
spec:
 type: NodePort
 ports:
  - targetPort: 80
    port: 80
    nodePort: 30008
 selector:
  author: intellqit
  type: proxy

Create pods from the above pod definition file
kubectl create -f pod-definition1.yml
Create the service from the above service definition file
kubectl create -f service.yml
Now nginx can be accesed from any of the slave
kubectl get nodes -o wide
Take the external ip of any of the nodes:30008



============================================================================
=========================================================================
Kubernetes Project
========================
This is a python based application which is used for accepting a vote
(voting app).This application accepts the vote and passes it to a
temporary db created using redis.From here the data is passed to a
worker application created using .net which anlysises the data and
stores them permananatly in a database created using postgres
From here the results can be seen on an application that is created 
using nodejs and this is called as resulta-app

To do this we will create 5 pod definition files
and 4 service files,2 services of type cluster ip for redis and postgres 
databases 2 services of type loadbalancer for python voting app and 
nodejs result app

Pod Definition Files
===========================

vim voting-app-pod.yml

---
apiVersion: v1
kind: Pod
metadata:
  name: voting-app-pod
  labels:
    name: voting-app-pod
    app: demo-voting-app
spec:
  containers:
    - name: voting-app
      image: dockersamples/examplevotingapp_vote
      ports:
        - containerPort: 80
...


vim result-app-pod.yml
---
apiVersion: v1
kind: Pod
metadata:
  name: result-app-pod
  labels:
    name: result-app-pod
    app: demo-voting-app
spec:
  containers:
    - name: result-app
      image: dockersamples/examplevotingapp_result
      ports:
        - containerPort: 80
...


vim worker-app-pod.yml

---
apiVersion: v1
kind: Pod
metadata:
  name: worker-app-pod
  labels:
    name: worker-app-pod
    app: demo-voting-app
spec:
  containers:
    - name: worker-app
      image: dockersamples/examplevotingapp_worker
...


vim redis-pod.yml
---
apiVersion: v1
kind: Pod
metadata:
  name: redis-pod
  labels:
    name: redis-pod
    app: demo-voting-app
spec:
  containers:
   - name: redis
     image: redis
     ports:
       - containerPort: 6379
...

vim postgres-pod.yml

---
apiVersion: v1
kind: Pod
metadata:
  name: postgres-pod
  labels:
    name: postgres-pod
    app: demo-voting-app
spec:
  containers:
    - name: postgres
      image: postgres:9.4
      ports:
        - containerPort: 5432
...


============================================================================
Service Defintion file
===============================
vim redis-service.yml
---
apiVersion: v1
kind: Service
metadata:
  name: redis-service
  labels:
    name: redis-service
    app: demo-voting-app
spec:
  ports:
    - port: 6379
      targetPort: 6379
  selector:
    name: redis-pod
    app: demo-voting-app
...

vim pod-service.yml
---
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
  labels:
    name: postgres-service
    app: demo-voting-app
spec:
  ports:
    - port: 5432
      targetPort: 5432
  selector:
    name: postgres-pod
    app: demo-voting-app
...

Note: Since "type" is not specified in the "spec" section they  will
be created as clusterIP




vim voting-app-service.yml
---
apiVersion: v1
kind: Service
metadata:
  name: voting-app-service
  labels:
    name: voting-app-service
    app: demo-voting-app
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 80
  selector:
    name: voting-app-pod
    app: demo-voting-app
...

vim result-app-service.yml
---
apiVersion: v1
kind: Service
metadata:
  name: result-app-service
  labels:
    name: result-app-service
    app: demo-voting-app
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 80
  selector:
    name: result-app-pod
    app: demo-voting-app
...


The above 2 service objects are created as LoadBalancer type ie
they can perfrom network load balancing where we can access the pod
from any slave and also a single public ip will be assigned for all
the salves




=============================================================================

To deploy the above project using docker stack
vim voting-app.yml
---
version: '3'

services:
 voting-app:
  image: dockersamples/examplevotingapp_vote
  ports:
   - 6060:80

 redis:
  image: redis
  ports:
   - 6379:6379

 worker-app:
  image: dockersamples/examplevotingapp_worker

 postgres:
  image: postgres
  environment:
   POSTGRES_PASSWORD: intelliqit
   POSTGRES_USER: myuser
   POSTGRES_DB: mydb
  ports:
   - 5432:5432

 result-app:
  image: dockersamples/examplevotingapp_result
  ports:
   - 7070:80


To deploy the above services
docker stack deploy -c voting-app.yml my-voting-app


To see the list of nodes where the stack services are running
docker stack ps my-voting-app

============================================================================
To deploy the above file in kubernetes using kompose

kompose up

To get the kubernetes file
kompose convert


==========================================================================
Day 22
==========================================================================

































